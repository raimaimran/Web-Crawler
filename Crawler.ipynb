{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "IR - A1 - L174349.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdfeH_hNCFCj"
      },
      "source": [
        "# **CRAWLER** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWZHHBC1GYnX"
      },
      "source": [
        "Intializing all libraries needed and parameters for crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR0LQgItCFCt"
      },
      "source": [
        "import os \n",
        "import random\n",
        "import sys\n",
        "from queue import Queue\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "from bs4 import BeautifulSoup\n",
        "from html import escape\n",
        "import urllib.robotparser\n",
        "import threading\n",
        "import time\n",
        "import datetime\n",
        "from heapq import heapify, heappush, heappop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt5vI38XCFCu"
      },
      "source": [
        "# Crawler Parameters\n",
        "THREADS = 3\n",
        "BACKQUEUES= THREADS *3\n",
        "FRONTQUEUES= 5\n",
        "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from \n",
        "CRAWLS = 1000\n",
        "\n",
        "__version__ = \"0.2\"\n",
        "USAGE = \"%prog [options] <url>\"\n",
        "VERSION = \"%prog v\" + __version__\n",
        "AGENT = \"%s/%s\" % (__name__, __version__)\n",
        "\n",
        "out_urls = [] #urls we get from a page\n",
        "visited_urls = [] #urls we have visited\n",
        "lock = threading.Lock()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZIPmTTbCFCv"
      },
      "source": [
        "# **FRONTIER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lcu63qbGfqD"
      },
      "source": [
        "Defining Frontier functions and logic for front queue and back queue operations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sOhMV5OVCFCv"
      },
      "source": [
        "def prioritizer(URL,f):\n",
        "    \"\"\"\n",
        "    Take URL and returns priority from 1 to F\n",
        "    Right now it like a stub function. \n",
        "    It will return a random number from 1 to f for given inputs. \n",
        "    \"\"\"\n",
        "    return random.randint(1, f)\n",
        "\n",
        "\n",
        "def addSecs(tm, secs):\n",
        "    fulldate = datetime.datetime(100, 1, 1, tm.hour, tm.minute, tm.second)\n",
        "    fulldate = fulldate + datetime.timedelta(seconds=secs)\n",
        "    return fulldate.time()\n",
        "\n",
        "\n",
        "class frontier:\n",
        "    def __init__(self):\n",
        "        self.seedURLs = [\"https://docs.oracle.com/en/\", \"https://www.oracle.com/corporate/\", \"https://en.wikipedia.org/wiki/Machine_learning\", \n",
        "                        \"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\", \"https://docs.oracle.com/middleware/jet210/jet/index.html\", \n",
        "                        \"https://en.wikipedia.org/w/api.php\", \"https://en.wikipedia.org/api/\", \"https://en.wikipedia.org/wiki/Weka_(machine_learning)\"]\n",
        "        self.front_queue = [Queue() for i in range(FRONTQUEUES)]\n",
        "        self.back_queue = [Queue() for i in range(BACKQUEUES)] \n",
        "        self.heap = []\n",
        "        heapify(self.heap)\n",
        "        self.curr_t = datetime.datetime.now().time()\n",
        "        for i in range(BACKQUEUES):\n",
        "          heappush(self.heap, (self.curr_t, i))\n",
        "    \n",
        "    def exists_in_fq(self, url, fq):\n",
        "      for q_element in list(fq.queue):\n",
        "        if q_element == url:\n",
        "          return True\n",
        "      return False\n",
        "        \n",
        "\n",
        "    def exists_in_bq(self, url, bq):\n",
        "      for q_element in list(bq.queue):\n",
        "        if q_element == url:\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "                \n",
        "    def add_seed_urls(self):\n",
        "        for url in self.seedURLs:\n",
        "            i = prioritizer(url, FRONTQUEUES)\n",
        "            self.front_queue[i-1].put(url)\n",
        "\n",
        "        for i in range(FRONTQUEUES):\n",
        "            self.add_to_backqueue() # fill backqueues\n",
        "            \n",
        "\n",
        "    def bq_empty(self):\n",
        "      for i in range(BACKQUEUES):\n",
        "        if self.back_queue[i].empty():\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "            \n",
        "    def add_URLs(self, URLs_list):\n",
        "        if len(URLs_list) == 0:\n",
        "          return\n",
        "          \n",
        "        for url in URLs_list:\n",
        "            i = prioritizer(url, FRONTQUEUES)\n",
        "            if not self.exists_in_fq(url, self.front_queue[i-1]):\n",
        "              self.front_queue[i-1].put(url)\n",
        "              if self.bq_empty(): # while backqueue empty -> add to bq\n",
        "                self.add_to_backqueue() # add some n links to bqs\n",
        "                #print(\"adding to bq\")\n",
        "\n",
        "\n",
        "    def get_URLs(self):\n",
        "\n",
        "      current_time = datetime.datetime.now().time() #getting current time\n",
        "      min_bq = heappop(self.heap)\n",
        "      last_fetched_bq = addSecs(min_bq[0], 15)\n",
        "      \n",
        "\n",
        "      while current_time < last_fetched_bq: # as soon as curr time > = last fetched time + 15 -> get data from bq\n",
        "            current_time = datetime.datetime.now().time()\n",
        "            #print(current_time)\n",
        "\n",
        "      URL = self.back_queue[min_bq[1]].get() # get url \n",
        "      \n",
        "      time = datetime.datetime.now().time() # adding current time to min heap\n",
        "      print(\"Access Time: \", time)\n",
        "      heappush(self.heap, (time, min_bq[1]))\n",
        "\n",
        "      return URL\n",
        "            \n",
        "        \n",
        "    def add_to_backqueue(self):\n",
        "        \n",
        "        fq_n = prioritizer(\" \", FRONTQUEUES)  # random fq selected\n",
        "        while self.front_queue[fq_n - 1].empty() == True: # if fq is empty -> check another fq \n",
        "            fq_n = prioritizer(\" \", FRONTQUEUES) \n",
        "\n",
        "        url = self.front_queue[fq_n - 1].get() \n",
        "        \n",
        "        url_split = url.split('/')\n",
        "\n",
        "        if not is_valid_url(url):\n",
        "          return\n",
        "        \n",
        "        added_to_bq = False\n",
        "        \n",
        "        while not added_to_bq:\n",
        "            for i in range(0, BACKQUEUES):  # check for any bq which might have same url\n",
        "                if not self.back_queue[i].empty():\n",
        "                    url2 = self.back_queue[i].get()\n",
        "                    url2_split = url2.split('/')\n",
        "                    if url_split[2] == url2_split[2] and not self.exists_in_bq(url, self.back_queue[i]):   # bq has url of that domain and url doesnt already exist\n",
        "                        #print(url_split[2])\n",
        "                        self.back_queue[i].put(url2)  # add url we dequeued\n",
        "                        self.back_queue[i].put(url)  # add new url\n",
        "                        added_to_bq = True\n",
        "                        #print(\"existed in some bq\")\n",
        "                        break\n",
        "                    else:\n",
        "                      self.back_queue[i].put(url2)\n",
        "\n",
        "            if not added_to_bq:\n",
        "                # else add to any free bq\n",
        "                for i in range(0, BACKQUEUES):\n",
        "                    if self.back_queue[i].empty():  # if bq empty can simply put in url\n",
        "                        self.back_queue[i].put(url)\n",
        "                        added_to_bq = True\n",
        "                        #print(\"bq empty\")\n",
        "                        break\n",
        "            if not added_to_bq:\n",
        "                self.front_queue[fq_n - 1].put(url)  # put url back in fq\n",
        "                #print(\"back to fq\")\n",
        "                added_to_bq = True\n",
        "                # no bq found break\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OekEnGyddUZa"
      },
      "source": [
        "# **FETCH URLS, PARSE AND FILTER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkQl2JYXzRGO"
      },
      "source": [
        "Here we are fetching a url, with the help of urllib we are extracting the contents of the html page. \n",
        "Once the contents are fetched, we parse and retrive urls from it. Using html escape library, we get absolute URLs\n",
        "Lastly, we check the robots.txt file under user agent * and only add links to out_urls if robots.txt allows   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UrSsVIFOZLf"
      },
      "source": [
        "class OpaqueDataException (Exception):\n",
        "    def __init__(self, message, mimetype, url):\n",
        "        Exception.__init__(self, message)\n",
        "        self.mimetype=mimetype\n",
        "        self.url=url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bDYBCg2dgbn"
      },
      "source": [
        "def is_valid_url(url):\n",
        "  if \"https\" in url:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def fetch_URL(url):\n",
        "  visited_urls.append(url)\n",
        "  url_split = url.split('/')\n",
        "  out_urls = []\n",
        "\n",
        "  try:\n",
        "    request = urllib.request.Request(url)\n",
        "    handle = urllib.request.build_opener()\n",
        "    rp = urllib.robotparser.RobotFileParser()\n",
        "    rp.set_url(\"https://\" + url_split[2] + \"/robots.txt\")\n",
        "    rp.read()\n",
        "  except IOError:\n",
        "    return None\n",
        "\n",
        "  request.add_header(\"User-Agent\", AGENT)\n",
        " \n",
        "  if handle:\n",
        "    try:\n",
        "      data=handle.open(request)\n",
        "      mime_type=data.info().get_content_type()\n",
        "      url=data.geturl()\n",
        "      if mime_type != \"text/html\":\n",
        "          raise OpaqueDataException(\"Not interested in files of type %s\" % mime_type,\n",
        "                                    mime_type, url)\n",
        "      content = data.read().decode(\"utf-8\", errors=\"replace\")\n",
        "      soup = BeautifulSoup(content, \"html.parser\")\n",
        "      tags = soup('a')\n",
        "    except urllib.error.HTTPError as error:\n",
        "      if error.code == 404:\n",
        "          print(sys.stderr, \"ERROR: %s -> %s\" % (error, error.url))\n",
        "      else:\n",
        "          print(sys.stderr, \"ERROR: %s\" % error)\n",
        "      tags = []\n",
        "    except urllib.error.URLError as error:\n",
        "      print(sys.stderr, \"ERROR: %s\" % error)\n",
        "      tags = []\n",
        "    except OpaqueDataException as error:\n",
        "      print(sys.stderr, \"Skipping %s, has type %s\" % (error.url, error.mimetype))\n",
        "      tags = []\n",
        "    for tag in tags:\n",
        "      href = tag.get(\"href\")\n",
        "      if href is not None:\n",
        "        url = urllib.parse.urljoin(url, escape(href))\n",
        "        if url not in out_urls and url not in visited_urls and rp.can_fetch(\"*\", url) and is_valid_url(url):\n",
        "          out_urls.append(url)\n",
        "    return out_urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Vl8nzGCFCz"
      },
      "source": [
        "# **RUN CRAWLER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZPiIWWMGzcy"
      },
      "source": [
        "Intializing the frontier(front queue and back queue), loading seed urls, defining thread task and running crawler "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZEOjWKUCFCz"
      },
      "source": [
        "# frontiers defined\n",
        "# fq and bq initialization \n",
        "url_frontier = frontier()\n",
        "url_frontier.add_seed_urls()\n",
        "url_frontier.add_to_backqueue()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3SeYU-qCFCz"
      },
      "source": [
        "# Theard task\n",
        "def crawler_thread_task(lock):\n",
        "  lock.acquire()\n",
        "  url = url_frontier.get_URLs()\n",
        "  print(\"Url Crawled:\" , url)\n",
        "\n",
        "  url_list = fetch_URL(url)\n",
        "  url_frontier.add_URLs(url_list)\n",
        "\n",
        "  #time.sleep(15) # wait time for crawler \n",
        "  lock.release()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25C1ShGLwUBx",
        "outputId": "912d878a-ab09-4662-ae49-7c0c0b0facdf"
      },
      "source": [
        "# start the threads \n",
        "\n",
        "threads = []\n",
        "for i in range(THREADS):\n",
        "    threads.append(threading.Thread(target=crawler_thread_task, args=(lock,)))\n",
        "\n",
        "while len(visited_urls) <= 1000:\n",
        "  for i in range(THREADS):\n",
        "    threads[i] = threading.Thread(target=crawler_thread_task, args=(lock,))\n",
        "\n",
        "  for i in range(THREADS):\n",
        "    threads[i].start()\n",
        "\n",
        "  for i in range(THREADS):\n",
        "    threads[i].join()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Access Time:  13:25:07.000051\n",
            "Url Crawled: https://docs.oracle.com/en/\n",
            "Access Time:  13:25:07.351333\n",
            "Url Crawled: https://en.wikipedia.org/api/\n",
            "Access Time:  13:25:07.461041\n",
            "Url Crawled: https://www.oracle.com/corporate/\n",
            "Access Time:  13:25:07.806973\n",
            "Url Crawled: https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\n",
            "Access Time:  13:25:09.744653\n",
            "Url Crawled: https://foundation.wikimedia.org/wiki/Developer_app_guidelines\n",
            "Access Time:  13:25:10.094047\n",
            "Url Crawled: https://developer.oracle.com/\n",
            "Access Time:  13:25:10.596464\n",
            "Url Crawled: https://profile.oracle.com/myprofile/account/create-account.jspx\n",
            "Access Time:  13:25:11.074765\n",
            "Url Crawled: https://academy.oracle.com/en/oa-web-overview.html\n",
            "Access Time:  13:25:11.439627\n",
            "Url Crawled: https://partner-finder.oracle.com/corporate/covid-19.html\n",
            "<ipykernel.iostream.OutStream object at 0x7f076c8cba50> ERROR: HTTP Error 302: The HTTP server returned a redirect error that would lead to an infinite loop.\n",
            "The last 30x error message was:\n",
            "Moved Temporarily\n",
            "Access Time:  13:25:22.000046\n",
            "Url Crawled: https://docs.oracle.com/middleware/jet210/jet/index.html\n",
            "Access Time:  13:25:22.453507\n",
            "Url Crawled: https://en.wikipedia.org/w/api.php\n",
            "Access Time:  13:25:24.929601\n",
            "Url Crawled: https://www.oracle.com/universal-menu/#products\n",
            "Access Time:  13:25:25.245146\n",
            "Url Crawled: https://go.oracle.com/legal/privacy/marketing-cloud-data-cloud-privacy-policy.html#adchoices\n",
            "Access Time:  13:25:25.516714\n",
            "Url Crawled: https://www.facebook.com/Oracle/\n",
            "Access Time:  13:25:26.470514\n",
            "Url Crawled: https://developer.oracle.com/startup/\n",
            "<ipykernel.iostream.OutStream object at 0x7f076c8cba50> ERROR: HTTP Error 404: Not Found -> https://developer.oracle.com/startup/\n",
            "Access Time:  13:25:26.883834\n",
            "Url Crawled: https://profile.oracle.com/corporate/contact/help.html\n",
            "<ipykernel.iostream.OutStream object at 0x7f076c8cba50> ERROR: HTTP Error 404: Not Found -> https://profile.oracle.com/corporate/contact/help.html\n",
            "Access Time:  13:25:27.166285\n",
            "Url Crawled: https://academy.oracle.com/partnernetwork/\n",
            "<ipykernel.iostream.OutStream object at 0x7f076c8cba50> ERROR: HTTP Error 404: Not Found -> https://academy.oracle.com/partnernetwork/\n",
            "Access Time:  13:25:27.471180\n",
            "Url Crawled: https://partner-finder.oracle.com/blockchain/\n",
            "<ipykernel.iostream.OutStream object at 0x7f076c8cba50> ERROR: HTTP Error 302: The HTTP server returned a redirect error that would lead to an infinite loop.\n",
            "The last 30x error message was:\n",
            "Moved Temporarily\n",
            "Access Time:  13:25:37.000054\n",
            "Url Crawled: https://www.csie.ntu.edu.tw/libsvmtools\n",
            "<ipykernel.iostream.OutStream object at 0x7f076c8cba50> ERROR: HTTP Error 404: Not Found -> https://www.csie.ntu.edu.tw/libsvmtools\n",
            "Access Time:  13:25:38.722483\n",
            "Url Crawled: https://en.wikipedia.org/wiki/Machine_learning\n",
            "Access Time:  13:25:39.223563\n",
            "Url Crawled: https://www.oracle.com/universal-menu/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHQSmN2BCFC0"
      },
      "source": [
        "## **End of Notebook**"
      ]
    }
  ]
}